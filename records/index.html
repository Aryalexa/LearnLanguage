<!doctype html>
<html>
<head>
    <title>TFG</title>

    <meta charset="utf-8" />
    <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css">
    <link rel="stylesheet" href="css/main.css">
    <link href='https://fonts.googleapis.com/css?family=Lato:400,300,700' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Lobster' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Dosis' rel='stylesheet' type='text/css'>

</head>

<body>

<h1 class="font-dosis text-center font-big green">TFG: Online Japanese learning</h1>
<div class="row">
<!-- ____________________________ RECORDS __________________________________ -->
  <div class="col-md-6 text-center top-marg">
    <h1 class="font-dosis font-big green">records</h1>
    <h4>Diario</h4>

    <img width="300" src="http://cdn.unbelievable-facts.com/wp-content/uploads/2016/02/shutterstock_140731432.jpg">

    <hr>
    <h3 class="font-dosis green">FECHAS</h3>
    <p>
      <b>Entrega memoria al director</b>: J 1 sept 2016 <br>
      <b>Entrega informe del director</b>: X 7 sept 2016<br>
      <b>Entrega de memoria en secretaría</b>: M 13 sept 2016 <br>
      <b>Publicación calendarios de presentaciones orales</b>: L 12 sept 2016<br>
      <b>Presentaciones</b>: V 16, L 19 sept 2016<br>
    </p>

    <hr>

    <p><a href="Antes_de_verano.html" >Antes de verano</a></p>
    <iframe width="600" src="Antes_de_verano.html"></iframe>

    <p><a href="7-7.html">7/7</a></p>
    <iframe width="600" src="7-7.html"></iframe>

    <p><a href="7-8.html">7/8</a></p>
    <iframe width="600" src="7-8.html"></iframe>

    <p><a href="7-9.html">7/9</a></p>
    <iframe width="600" src="7-9.html"></iframe>

    <p><a href="7-15.html">7/15</a></p>
    <iframe width="600" src="7-15.html"></iframe>

    <p><a href="7-26.html">7/26</a></p>
    <iframe width="600" src="7-26.html"></iframe>

  </div>

<!-- _______________________________ INFO __________________________________ -->

  <div class="col-md-6 top-marg">

    <div class="text-center">
      <h1 class="font-dosis font-big green">info</h1>
      <h4>Información</h4>
      <img width="300" src="https://upload.wikimedia.org/wikipedia/commons/1/18/Plants-Sicily-bjs-2.jpg">
    </div>

    <hr>
    <h3 class="font-dosis green">DESCRIPCIÓN</h3>
    <p>
      <b>Título</b>: Aprendizaje de japonés online <br>
      <b>Directores</b>: Adrián Riesco y Enrique Martín <br>
      <b>Estudiante</b>: Mayra Alexandra Castrosqui Florian <br>
    </p>
    <p>
      Se propone implementar una aplicación para dispositivos móviles para el
      estudio autónomo del japonés. En concreto, se usará el micrófono para la
      <b>práctica de la pronunciación</b>, una característica inédita en aplicaciones
      no comerciales y que requerirá el tratamiento del audio. Asimismo, se
      aprovechará la pantalla táctil para la <b>práctica de la caligrafía</b>. Por último,
      se propone generar taxomomías de manera automática para sistematizar el
      aprendizaje de <b>vocabulario</b>. En concreto, es especialmente interesante la
      práctica de la pronunciación como objetivo tanto en el campo de la Informática
      como de las Matemáticas.
    </p>
    <p>
      Esta aplicación requerirá que el estudiante:
    </p>
    <ul>
      <li>Elimine el ruido, para lo que deberá primero analizar la onda resultante
      del audio, identificar y eliminar el ruido de fondo, eliminar las frecuencias
      fuera del alcance humano para compresión y comparar el resultado final con
      una "onda maestra” para asegurarse de la corrección del resultado.</li>
      <li>Compare la onda introducida con la de referencia. Para ello es necesario
      crear un espectrograma y normalizarlo para identificar diferencias aceptables
      en velocidad, por un lado, y en volumen, por otro.</li>
      <li>Realice la comparación final basándose en matrices. En principio pretendemos
      utilizar la similitud coseno, pero puede resultar interesante implementar
      otras variantes y comparar los resultados.</li>
    </ul>

<!-- __________________________ INFO: APRENDER _____________________________ -->

    <hr>
    <h3 class="font-dosis green">Para aprender Android</h3>
    En el sitio oficial <a href="https://developer.android.com/training/index.html">developer.android.com/traning</a>
    <br>
    Libros (bucm online):
    <a href="http://cisne.sim.ucm.es/record=b3249256~S6*spi">1</a>,
    <a href="http://cisne.sim.ucm.es/record=b3292591~S6*spi">2</a>


<!-- __________________________ INFO: APLICACION ____________________________ -->

    <hr>
    <h3 class="font-dosis green">Detalles de la aplicación</h3>
    * Elegir nivel de API, Minimum SDK (API 15)
    <br>
    la más baja de Ice Cream Sándwich, que unifica tableta y móvil: API 14.
    La 14 aparece como obsoleta. Puede ser más interesante escoger la 15,
    que es Ice Cream Sandwich 4.0.3 y 4.0.4.
    La diferencia será mínima y no está marcada como obsoleta.
    <br>
    * targetSdkVersion 23
    <br>
    * Móvil a emular, piensa en un Samsung o Nexus moderno.
    <br>
    * Móvil usado: Samsung SM-G800F (Android 4.4.2, API 19)


<!-- ____________________________ INFO: SONIDO _____________________________ -->

    <hr>
    <h3 class="font-dosis green">FUN: 1. Practicar pronunciación</h3>
    <p>
      * Bibliotecas de Android que pueden ser interesante para esta fase del proyecto:
      <a href="http://developer.android.com/reference/android/media/AudioRecord.html">AudioRecord</a>,
      <a href="http://developer.android.com/reference/android/media/audiofx/Equalizer.html">Equalizer</a>
      <br>
      Además, una externa que también puede estar bien:
      <a href="http://commons.apache.org/proper/commons-math/userguide/index.html">apache commmons-math</a>
      <br>
      * Usar es la biblioteca TarsosDSP:
      <a href="http://husk.eecs.berkeley.edu/courses/cs160-sp14/index.php/Sound_Programming">Sound Programming</a>
      Esta biblioteca tiene funciones para filtros, para la transformada de Fourier
      y para pintar el espectrograma, que es todo lo que nos interesa en los pasos
      que tenemos que dar ahora.
      <br>
      Tienes los filtros en
      <a href="https://github.com/JorenSix/TarsosDSP/tree/71ae353871a75d916a642dbb7377fead9d175a68/src/core/be/tarsos/dsp/filters">filtros</a>
      <br>
      Tienes un ejemplo de FFT y su dibujo a partir de la línea 197 de
      <a href="https://github.com/JorenSix/TarsosDSP/blob/master/src/examples/be/tarsos/dsp/example/Spectrogram.java">Spectograma.java</a>
      <br>
      En <a href="http://husk.eecs.berkeley.edu/courses/cs160-sp14/index.php/Sound_Programming">Sound Programming</a>  había
      un fragmento de código para obtener una grabación en formato PCM (array de valores) y
      luego aplicar un filtro paso bajo a ese array para obtener otro solo con las frecuencias
      menores a 1000 Hz. En nuestro caso queremos un filtro paso banda pero el código es similar.
      <br>
      La página con el filtro paso banda: <a href="https://github.com/JorenSix/TarsosDSP/issues/68">issue68</a>.
      Y con la FFT:
      <a href="https://github.com/JorenSix/TarsosDSP/blob/bdf2c4ffcfee46a2a6bea07f9e634450692d7e1d/src/core/be/tarsos/dsp/util/fft/FFT.java">FFT.java</a>

    </p>
    <br><br>

    <p>
      <b>DSP</b> (digital signal processing): numerical manipulation of signals, usually
      with the intention to measure, filter, produce or compress continuous analog signals.
      <br>
      Una de las más importantes transformadas es la transformada de Fourier discreta (<b>TFD</b>).
      Esta transformada convierte la señal del dominio del tiempo al dominio de la frecuencia.
      La TFD permite un análisis más sencillo y eficaz sobre la frecuencia, sobre todo en
      aplicaciones de eliminación de ruido y en otros tipos de filtrado.
      <br>
      <b>Filtros</b>:
      <ul>
        <li>filtros de paso bajo (permitir el paso de las frecuencias más bajas y atenuar las frecuencias más altas)</li>
        <li>de paso alto HPF (en su respuesta en frecuencia se atenúan las componentes de baja frecuencia pero no las de alta frecuencia)</li>
        <li>de paso banda (deja pasar un determinado rango de frecuencias de una señal y atenúa el paso del resto)</li>
        <li>de rechazo de banda</li>
        <li>...</li>
      </ul>
    </p>
    <h4 class="purple">GUIDE(<a href="http://stackoverflow.com/questions/17010516/how-to-detect-how-similar-a-speech-recording-is-to-another-speech-recording">read</a>)</h4>
    <p>
      Palabras clave: openCV, binary large object (blop)
    </p>
    <p>
      A. specific algorithms used by speech-recognition in fact are nearly the opposite of what you would like to use here.
      <br>
      B. TO DO:
      <ul>
        <li> <!-- ___________________ 1 __________________ -->
          <b>1. Record audio in a format for pyhton use it</b>
          <br>
          <b>2. Eliminate background noise con python</b>
          we have a chunk of audio (without any filtering, acquired from a microphone).
          The first step is to eliminate background noise.
          <br>
          Filtra el audio usando el modulo de scipy <a href="http://docs.scipy.org/doc/scipy/reference/signal.html">aquí</a>.
          Hay un montón de frecuencias que el microfono
          recoge y que no son útiles para categorizar el speech.
          Sugiero un filtro de Bessel o de Butterworth para segurar que tu waveform es preservada
          por el filtro. Las frecuencias fundamentales del speech cotidiano están generalmente
          entre 800 y 2000 Hz (*), así que un corte razonable sería como de 300 a 4000 Hz, para
          asegurarte de no perder nada.
          <br>
          Busca la porción menos activa del speech y asume que es una representación razonable del
          background noise. Ahora queremos correr una serie de transformaciones de fourier a lo largo de
          los datos (o generar un espectograma) y encontrar la parte de la grabación del speech
          que tenga la respuesta de frecuencia media más baja. Una vez hecho el snapshot, debemos
          sustraerlo de todo el resto de puntos en la muestra de audio.
          <br>
          Conseguimos así un archivo de audio que es en su mayor parte el speech del usuario y está listo
          para ser comparado a otro archivo que ha pasado por el mismo proceso.
          'Now, we want to actually clip the sound and compare this clip to
          some master clip.'
        </li>
        <li>
          We want to come up with a distance metric between two speech patterns. One way:
          Let's assume we have the output of part one and some master file that has been through similar processing.
          <br>
          Gereramos un espectograma del archivo de audio en cuestión. El resultado será en última
          instancia una imagen que podrá ser representada como un array 2d con valores de respuesta de frecuencia.
          Un espectograma es esencialmente una transformación de fourier sobre el tiempo donde el color corresponde
          con la intensidad.
          <br>
          Usamos OpenCV (*) para ejecutar detección de blob en el espectograma. Esto buscará un gran
          colorido blob en el medio del especctograma y te dará algunos limites. Te dará una versión más
          dispersa del array2D que representa el speech.
          <br>
          Normalizamos la velocidad: Basically you want to
          stretch out the shorter version by multiplying it's time axis by some constant
          that's just the ratio of the lengths of your two blobs.
          <br>
          Normalizamos el volumen: (maximum and minimum intensity).
        </li>
        <li>
          Ya tenemos los dos arrays2D con toda la información útil para compararlos directamente.
          <br>
          Recomendado using a metric like Cosine Similarity to determine the difference between your two blobs, mejorable.
        </li>
      </ul>
    </p>

    <h4>Grabar</h4>
    <h4>Visualizar</h4>
    <h4>Comparar</h4>

    *

<!-- _________________________ INFO: CALIGRAFIA ____________________________ -->

<!-- ________________________ INFO: VOCABULARIO ____________________________ -->

<!-- __________________________ IDEAS _____________________________ -->

    <hr>
    <h3 class="font-dosis green">I D E A S</h3>
    Multiplataforma? (pregunta del trib)
    <br>
    Conclusiones, aportaciones
    <br>
    <br>
    <a href="#">link</a> <!-- EXAMPLE LINK -->
<!-- __________________________ PROB  _____________________________ -->

    <hr>
    <h3 class="font-dosis green">Problemas detectados</h3>
    Prob:
    <br>
    Sol:
    <br><br>
    Prob:
    <br>
    Sol: <a href="#">link</a> <!-- EXAMPLE LINK -->

  </div>

</div>
</body>
</html>
