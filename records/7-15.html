<!doctype html>
<html>
<head>
    <title>Día 7/15, 7/16, 7/17</title>

    <meta charset="utf-8" />
    <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css">
    <link rel="stylesheet" href="css/main.css">
    <link href='https://fonts.googleapis.com/css?family=Lato:400,300,700' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Lobster' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Dosis' rel='stylesheet' type='text/css'>

</head>

<body>
<div class="all">
  <h2 class="font-dosis green">Hoy: 7/15, 7/16, 7/17</h2>

  <h3>Día anterior</h3>
  <ul>
    <li>Recaudación de información</li>
    <li>El desarrollo de fragments con android.developer... buscar en otro sitio.</li>
    <li><a href="#">link</a></li>
  </ul>

  <h3>Planes para hoy</h3>
  <ul>
    <li>pronunciación</li>
    <li></li>
  </ul>

  <h3>records</h3>
  <ul>
    <li> <a href="https://es.wikipedia.org/wiki/Procesamiento_digital_de_señales">DSP</a>
      <p>
        <b>DSP</b> (digital signal processing): numerical manipulation of signals, usually
        with the intention to measure, filter, produce or compress continuous analog signals.
        <br>
        Una de las más importantes transformadas es la <a href="https://es.wikipedia.org/wiki/Transformada_de_Fourier_discreta">transformada de Fourier discreta</a> (<b>TFD</b>).
        Esta transformada convierte la señal del dominio del tiempo al dominio de la frecuencia.
        La TFD permite un análisis más sencillo y eficaz sobre la frecuencia, sobre todo en
        aplicaciones de eliminación de ruido y en otros tipos de filtrado.
        <br>
        <b>Filtros</b>:
        <ul>
          <li>filtros de paso bajo (permitir el paso de las frecuencias más bajas y atenuar las frecuencias más altas)</li>
          <li>de paso alto HPF (en su respuesta en frecuencia se atenúan las componentes de baja frecuencia pero no las de alta frecuencia)</li>
          <li>de paso banda (deja pasar un determinado rango de frecuencias de una señal y atenúa el paso del resto)</li>
          <li>de rechazo de banda</li>
          <li>...</li>
        </ul>
      </p>
    </li>
    <li> <a href="https://es.wikipedia.org/wiki/Procesamiento_digital_de_voz">Procesamiento digital de voz</a> o speech processing
      y de <a href="https://en.wikipedia.org/wiki/Audio_signal_processing">audio</a>, técnicas:
      <ul>
        <li>echo </li>
        <li>flanger: efecto de sonido que produce un característico sonido metalizado oscilante</li>
        <li>phaser: efecto de sonido similar al flanger. La señal se dobla y luego se le aplica un retraso. </li>
        <li>chorus</li>
        <li>equalization<a href="https://en.wikipedia.org/wiki/Equalization_(audio)">+</a>: ajustar el balance entre las componentes de frecuencia en la señal electrónica.</li>
        <li>filtering</li>
        <li>overdrive</li>
        <li>pitch shift</li>
        <li>time stretching</li>
        <li>resonators</li>
        <li>robotic voice effects</li>
        <li>synthesizer</li>
        <li>modulation</li>
        <li>compression</li>
        <li>3D audio effects</li>
        <li>reverse echo</li>
        <li>active noise control</li>
        <li>wave field synthesis</li>
      </ul>
    </li>
    <li> <a href="http://stackoverflow.com/questions/6318168/how-to-compare-word-pronounce">StackOverflow - how to compare word pronounce</a>.
      Simply compute the MFCC (<a href="http://en.wikipedia.org/wiki/Mel-frequency_cepstrum">wiki-MFC</a>) of the recording, and then look at
      something simple like the correlation between the recording and the average coefficients of that word being pronounced by a native speaker.
      The MFCC will transform the audio into a space where euclidean distance corresponds more closely with perceptual difference. [keep reading,
      tendría que buscar MFC con android]
    </li>
    <li> <a href="http://stackoverflow.com/questions/33755496/in-android-is-there-a-way-to-compare-two-audio-files-for-similarity-between-them">StackOverflow - in android, compare audio files for similarity betweeen them</a>
      use FFT (fast Fourier transform), the differentiation of voice is not frecuences, is modulation. 'Speech-Recognition' (includes FFT modulation).
      Además, <a href="http://khurramitdeveloper.blogspot.com.es/2014/01/compare-two-sounds-in-android.html">musicg</a> es una app para analisis de audio.
      (Google's open source lib musicg API: <a href="http://code.google.com/p/musicg/">src</a>, it's Java so it works in Android and it gives similarity metrics for two audio files.)
    </li>
    <li> <a href="http://stackoverflow.com/questions/11705224/matching-two-audio-files">StackOverflow - matching-two-audio-files</a>
      (Dog barks).- 1. Use an FFT (Fast Fourier Transform) to get the spectrum of the bark and compare the spectrum's. You might be able to define some filters to help the analysts.-
      2. FFT is only the tip of the iceberg here. I have implemented audio fingerprinting that uses FFT just in one part of the process - and it ONLY matches two audibly same pieces
      of sound, your problem is far greater than that. In any case, you'll probably end up using some server-side solution, where your android device will only GET the
      audio and send it to the server, which will do the comparison. Investigate shazaam, playkontrol or soundhound...-
      <br>
      3. look into how audio fingerprinting works. This paper is an excellent start written by the creators of shazam:
      <a href="http://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf">wang03-shazam.pdf</a>. +  Look into is how the FFT works.
      Here's a tutorial with code that I wrote for pitch tracking, which is one way to use the FFT :
      <a href="http://blog.bjornroche.com/2012/07/frequency-detection-using-fft-aka-pitch.html">tut</a>.
      <br>
      RELEER
    </li>
    <li> <a href="http://stackoverflow.com/questions/15653466/simplest-algorithm-of-measuring-how-similar-of-two-short-audio">StackOverflow - simplest-algorithm-of-measuring-how-similar-of-two-short-audio</a> .
      The similarity between two sequences of variable length can be efficiently calculated with DTW:
      <a href="http://en.wikipedia.org/wiki/Dynamic_time_warping">Dynamic time warping</a>.
      <br>
      It's reasonable to split the audio on frames and turn it into 2-D vector of features where
      for each frame you have an array of values(features) corresponding to the different frequency bands.
      <br>
      For speech, it's better to calculate mel-frequency cepstrum <a href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/">MFC</a>.
      <br>
      One existing libraries for mel frequency features is a 'speech recognition' toolkit  <a href="http://cmusphinx.sourceforge.net">CMUSphinx</a>.
    </li>

    <li> <a href="http://stackoverflow.com/questions/23422939/compare-two-voice-in-android">StackOverflow - compare two voice in android</a>
      (security question). Speech recognition apps:
      <br>
      An OSS library called Alize (written in C++, under LGPL license)
      which uses an algorithm called MFCC (Mel Frequency Cepstrum Coefficients).
      MFCC is known to bring excellent results. Expect a steep learning curve as
      this software is aimed at researchers willing to improve the state-of-the-art
      on this topic and the vocabulary used is very specific.
      <br>
      An OSS library named Recognito (Java, Apache 2.0) aimed at regular developers
      so you should be able to test it in a matter of minutes. Author says: The lib is very young
      and I first focused on it's API before improving the algorithms. The algorithm
      I use for the moment is called Linear Predictive Coding (LPC) and is known
      to bring good results. I'm currently in the process of releasing a new ve
      rsion including a likelihood coefficient in the match results. MFCC implementation
      is on the road map. There is plenty of javadoc and the code should be very
      straightforward... <a href="https://github.com/amaurycrickx/recognito">Github - recognito</a>.
      <br>
      Recognito has a dependency on javax.sound packages for audio file handling.
      You may want to check this post for what it takes to use it in Android:
      <a href="http://stackoverflow.com/questions/22443124/voice-matching-in-android/22445304#22445304">++</a>
    </li>
    <li> <a href="http://stackoverflow.com/questions/10058469/using-fft-to-compare-two-audio-files-and-then-realtime-comparison">StackOverflow - using-fft-to-compare-two-audio-files-and-then-realtime-comparison</a>
      Should it do voice recognition?
      <br>
      If you just want have a simple measure for simliarity use cross correlation (ask google).
      An FFT might come into play once you know what you want to do and you know
      that you really need it; perhaps for applying some psychcoacustic model.
      But even then a simple haar transform might be more sufficient,
      simpler and faster, especially on moblie devices.
    </li>
    <li> <a href="http://stackoverflow.com/questions/20076536/how-to-compare-two-voices-in-android">StackOverflow - how to compare two voices in android</a>.
      nah
    </li>
    <li> <a href="http://stackoverflow.com/questions/14213090/compare-two-sound-in-android">StackOverflow - compare two sound in android</a>.
      Rate the similarity like Good, Very Good, ... Use marsyas (<a href="https://sourceforge.net/projects/marsyas/">1</a>,
      <a href="http://marsyas.info">2</a>). Marsyas even provides a sample android application.
      After getting a proper signal analysis framework, you need to analyse your signal.
      <br>
      A. For example, the <a href="https://github.com/coatandhat/aimc">AimC</a> implementation
      for marsyas can be used to compare voice. Recommended installing marsyas on your computer
      and fiddle with the python example scripts. This network takes your audio data and
      transforms it as it would be processed by a human ear. After that it uses vector
      quantization to reduce the many possible vectors to very specific codebooks with 200 entries.
      You can then translate the output of the network to readable characters (utf8 for example),
      which you then can compare using something like string edit distances (e.g. Levenshtein distance).
      <br>
      B. Use MFCC (Mel Frequency Cepstral Coefficients) for speech recognition which marsyas
      supports as well (<a href="https://sourceforge.net/p/marsyas/code/5002/tree/trunk/src/marsyas/MFCC.cpp">here</a>)
      and use something, for example <a href="https://sourceforge.net/p/marsyas/code/5002/tree/trunk/src/marsyas/DTW.cpp">Dynamic Time Warping</a>,
      to compare the outputs.
      This <a href="http://www.ijest.info/docs/IJEST10-02-12-198.pdf">document</a> describes the process pretty well.
    </li>
    <li> <a href="http://stackoverflow.com/questions/17010516/how-to-detect-how-similar-a-speech-recording-is-to-another-speech-recording">StackOverflow - detect how similar a speech recording is to anothe one</a>.
      [SPECIAL INFO].
      <br>
      1. specific algorithms used by speech-recognition in fact are nearly the opposite of what you would like to use here.
      <br>
      2. TO DO:
      <br>
      - we have a chunk of audio (without any filtering, acquired from a microphone).
      The first step is to eliminate background noise.
      <br>
      Filtra el audio usando el modulo de scipy <a href="http://docs.scipy.org/doc/scipy/reference/signal.html">aquí</a>.
      Hay un montón de frecuencias que el microfono
      recoge y que no son útiles para categorizar el speech.
      Sugiero un filtro de Bessel o de Butterworth para segurar que tu waveform es preservada
      por el filtro. Las frecuencias fundamentales del speech cotidiano están generalmente
      entre 800 y 2000 Hz (*), así que un corte razonable sería como de 300 a 4000 Hz, para
      asegurarte de no perder nada.
      <br>
      Busca la porción menos activa del speech y asume que es una representación razonable del
      background noise. Ahora queremos correr una serie de transformaciones de fourier a lo alrgo de
      los datos (o generar un espectograma) y encontrar la parte de la grabación del speech
      que tenga la respuesta de frecuencia media más baja. Una vez hecho el snapshot, debemos
      sustraerlo de todo el resto de puntos en la muestra de audio.
      <br>
      Conseguimos así un archivo de audio que es en su mayor parte el speech del usuario y está listo
      para ser comparado a otro archivo que ha pasado por el mismo proceso.
      'Now, we want to actually clip the sound and compare this clip to
      some master clip.'
      <br>
      - We want to come up with a distance metric between two speech patterns. One way:
      Let's assume we have the output of part one and some master file that has been through similar processing.
      <br>
      Gereramos un espectograma del archivo de audio en cuestión. El resultado será en última
      instancia una imagen que podrá ser representada como un array 2d con valores de respuesta de frecuencia.
      Un espectograma es esencialmente una transformación de fourier sobre el tiempo donde el color corresponde
      con la intensidad.
      <br>
      Usamos OpenCV (*) para ejecutar detección de blob en el espectograma. Esto buscará un gran
      colorido blob en el medio del especctograma y te dará algunos limites. Te dará una versión más
      dispersa del array2D que representa el speech.
      <br>
      Normalizamos la velocidad: Basically you want to
      stretch out the shorter version by multiplying it's time axis by some constant
      that's just the ratio of the lengths of your two blobs.
      <br>
      Normalizamos el volumen: (maximum and minimum intensity).
      <br>
      - Ya tenemos los dos arrays2D con toda la información útil para compararlos directamente.
      <br>
      Recomendado using a metric like Cosine Similarity to determine the difference between your two blobs, mejorable.
      <br>



    </li>
    <li> <a href="http://stackoverflow.com/questions/6979352/how-can-i-compare-2-audio-files-programmatically">StackOverflow - how can i compare 2 audio files programmatically</a>
    </li>
    <li> <a href="http://stackoverflow.com/questions/5628863/how-to-compare-spoken-audio-against-reference-recording-language-learning">StackOverflow - how-to-compare-spoken-audio-against-reference-recording-language-learning</a>
    </li>
    <li> <a href="http://stackoverflow.com/questions/7078226/comparing-audio-recordings">StackOverflow - comparing-audio-recordings</a>
    </li>

  </ul>

  <p>
    * <a href="https://github.com/ispikit/ispikit-cordova">Github - Cordova plugin for Ispikit pronunciation assessment and speech recognition</a>, (.java descargado)

  </p>

</div>

</body>
</html>
